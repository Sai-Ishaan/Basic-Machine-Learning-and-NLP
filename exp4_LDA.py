# -*- coding: utf-8 -*-
"""exp4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b6yYVv5lCjoht_XgtxBFectpVA6vKNvU
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('pokemon.csv')

# Basic information about data
print("Basic information about the dataset:")
print(df.info())

# Descriptive statistics
print("\nDescriptive statistics:")
print(df.describe())

# Visualize relationships (e.g., pair plots)
sns.pairplot(df)
plt.show()

import numpy as np
import pandas as pd
from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X = df[['sp_attack', 'sp_defense', 'speed']]  # Adjust the feature names as needed

# Create a binary classification task (e.g., Legendary vs. non-Legendary)
y_binary = df['is_legendary'].astype(int)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)
# Initialize the Perceptron model
perceptron = Perceptron(max_iter=1000, random_state=42)

# Train the model
perceptron.fit(X_train, y_train)

# Predict on the test set
y_pred = perceptron.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Check if the Perceptron converged (accuracy is 100%)
if accuracy == 1.0:
    print("The Pokemon dataset is linearly separable.")
else:
    print("The Pokemon dataset is not linearly separable.")

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset (you can replace this with your own dataset)
iris = load_iris()
X, y = iris.data, iris.target

# Create a binary classification task (e.g., setosa vs. non-setosa)
y_binary = np.where(y == 0, 1, 0)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)

# Initialize the Perceptron model
perceptron = Perceptron(max_iter=1000, random_state=42)

# Train the model
perceptron.fit(X_train, y_train)

# Predict on the test set
y_pred = perceptron.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Check if the Perceptron converged (accuracy is 100%)
if accuracy >= 0.9:
    print("The dataset is linearly separable.")
else:
    print("The dataset is not linearly separable.")

from sklearn.model_selection import train_test_split
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
iris = datasets.load_iris()
df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],
                  columns=iris['feature_names'] + ['target'])
df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)
df.columns = ['s_length', 's_width', 'p_length', 'p_width', 'target', 'species']

X = df[['s_length', 's_width', 'p_length', 'p_width']]
y = df['species']
model = LinearDiscriminantAnalysis()
model.fit(X, y)

print(df.head(10))

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
print(f"Mean accuracy: {np.mean(scores):.2f}")

#Bank Note authentication dataset
cols = ['Variance','Skewness','Curtosis', 'Entropy','is_Authentic']
df1 = pd.read_csv('data_banknote_authentication.csv', names=cols)
print(df1.head(10))

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X = df1.drop(columns=['is_Authentic'])
y = df1['is_Authentic']
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy score: {accuracy:.2f}")

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Split data into features (X) and target (y)
X = df1.drop(columns=['is_Authentic'])
y = df1['is_Authentic']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and fit the LDA model
lda_model = LinearDiscriminantAnalysis()
lda_model.fit(X_train, y_train)

X_train_lda = lda_model.transform(X_train)
X_test_lda = lda_model.transform(X_test)

# Initialize a logistic regression model on the transformed data
model_lda = LogisticRegression()
model_lda.fit(X_train_lda, y_train)

# Make predictions on the test data
y_pred_lda = model_lda.predict(X_test_lda)

accuracy_lda = accuracy_score(y_test, y_pred_lda)
print(f"Accuracy score after LDA: {accuracy_lda:.2f}")

#Decision Tree
#Scatter Plot
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
n_classes = 3
plot_colors = "bry"
plot_step = 0.02

for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):
    X = iris.data[:, pair]
    y = iris.target
    clf = DecisionTreeClassifier().fit(X, y)

    plt.subplot(2, 3, pairidx + 1)
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
    plt.xlabel(iris.feature_names[pair[0]])
    plt.ylabel(iris.feature_names[pair[1]])
    plt.axis("tight")

    for i, color in zip(range(n_classes), plot_colors):
        idx = np.where(y == i)
        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i], cmap=plt.cm.Paired)

plt.suptitle("Decision surface of a Decision Tree using paired features")
plt.legend()
plt.show()

plt.scatter(X[:, 0 ],X[:,1], c=y, cmap = plt.cm.Paired)
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.title("Scatter Plot of Sepal Length vs Sepal Width")
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
cls = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'Class']
dataset = pd.read_csv(url, names=cls)

X = dataset.iloc[:, 0:4].values
y = dataset.iloc[:, 4].values

sc = StandardScaler()
X = sc.fit_transform(X)
le = LabelEncoder()
y = le.fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

lda = LinearDiscriminantAnalysis(n_components=2)
X_train = lda.fit_transform(X_train, y_train)
X_test = lda.transform(X_test)

plt.scatter(
    X_train[:,0],X_train[:,1],c=y_train,cmap='rainbow',
  alpha=0.7,edgecolors='b')

classifier = RandomForestClassifier(max_depth=2, random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
print('Accuracy : ' + str(accuracy_score(y_test, y_pred)))
conf_m = confusion_matrix(y_test, y_pred)
print(conf_m)

X = df1.drop(columns=['is_Authentic'])
y = df1['is_Authentic']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and fit the LDA model
lda_model = LinearDiscriminantAnalysis()
lda_model.fit(X_train, y_train)

X_train_lda = lda_model.transform(X_train)
X_test_lda = lda_model.transform(X_test)

# Initialize a logistic regression model on the transformed data
model_lda = LogisticRegression()
model_lda.fit(X_train_lda, y_train)

# Make predictions on the test data
y_pred_lda = model_lda.predict(X_test_lda)

accuracy_lda = accuracy_score(y_test, y_pred_lda)
print(f"Accuracy score after LDA: {accuracy_lda:.2f}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix


cols = ['Variance','Skewness','Curtosis', 'Entropy','is_Authentic']
df1 = pd.read_csv('data_banknote_authentication.csv', names=cols)
print(df1.head(10))
# Split data into features (X) and target (y)
X = df1.drop(columns=['is_Authentic'])
y = df1['is_Authentic']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and fit the LDA model
lda_model = LinearDiscriminantAnalysis()
lda_model.fit(X_train, y_train)

X_train_lda = lda_model.transform(X_train)
X_test_lda = lda_model.transform(X_test)

# Initialize a logistic regression model on the transformed data
model_lda = LogisticRegression()
model_lda.fit(X_train_lda, y_train)

# Make predictions on the test data
y_pred_lda = model_lda.predict(X_test_lda)

accuracy_lda = accuracy_score(y_test, y_pred_lda)
print(f"Accuracy score after LDA: {accuracy_lda:.2f}")
X = df1.drop(columns=['is_Authentic'])
y = df1['is_Authentic']

plt.scatter(
    X_train[:,0],X_train[:,1],c=y_train,cmap='rainbow',
  alpha=0.7,edgecolors='b')

classifier = RandomForestClassifier(max_depth=2, random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
print('Accuracy : ' + str(accuracy_score(y_test, y_pred)))
conf_m = confusion_matrix(y_test, y_pred)
print(conf_m)

